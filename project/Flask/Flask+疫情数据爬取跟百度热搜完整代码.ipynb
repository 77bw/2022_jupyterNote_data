{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬取疫情数据并同步到sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as rq\n",
    "import pymysql\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_today = \"https://view.inews.qq.com/g2/getOnsInfo?name=disease_h5\"\n",
    "url_last = \"https://view.inews.qq.com/g2/getOnsInfo?name=disease_other\"\n",
    "def gethtml(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"}\n",
    "    req = rq.Request(url, headers=headers)\n",
    "    res = rq.urlopen(req)\n",
    "\n",
    "    html = res.read().decode(\"utf-8\")\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_history(url_last, url_today):\n",
    "    # 历史数据\n",
    "    html_last = gethtml(url_last)\n",
    "    data_last = json.loads(html_last)\n",
    "    #dict_keys(['chinaDayList', 'chinaDayAddList', 'dailyNewAddHistory', 'dailyHistory', 'wuhanDayList', 'articleList', 'provinceCompare', 'cityStatis', 'nowConfirmStatis'])\n",
    "    data_last = json.loads(data_last[\"data\"])\n",
    "\n",
    "    # 当天数据\n",
    "    html_today = gethtml(url_today)\n",
    "    data_today = json.loads(html_today)\n",
    "    # dict_keys(['lastUpdateTime', 'chinaTotal', 'chinaAdd', 'isShowAdd', 'showAddSwitch', 'areaTree'])\n",
    "    data_today = json.loads(data_today[\"data\"])\n",
    "    # print(data_today.keys())\n",
    "\n",
    "    history_data = {}\n",
    "    for every_data in data_last[\"chinaDayList\"]:\n",
    "        date = \"2021.\" + every_data[\"date\"]\n",
    "        tup = time.strptime(date, \"%Y.%m.%d\") #匹配时间\n",
    "        date = time.strftime(\"%Y-%m-%d\", tup)  # 改变时间格式,不然插入数据库会报错，数据库是datetime类型\n",
    "\n",
    "        confirm = every_data[\"confirm\"]\n",
    "        suspect = every_data[\"suspect\"]\n",
    "        dead = every_data[\"dead\"]\n",
    "        heal = every_data[\"heal\"]\n",
    "        # nowConfirm = every_data[\"nowConfirm\"]\n",
    "\n",
    "        history_data[date] = {\"confirm\":confirm, \"suspect\":suspect, \"dead\":dead, \"heal\":heal}\n",
    "\n",
    "    for every_data_add in data_last[\"chinaDayAddList\"]:\n",
    "        date_add = \"2021.\" + every_data_add[\"date\"]\n",
    "        tup_add = time.strptime(date_add, \"%Y.%m.%d\")  #匹配时间\n",
    "        date_add = time.strftime(\"%Y-%m-%d\", tup_add)  # 改变时间格式,不然插入数据库会报错，数据库是datetime类型\n",
    "\n",
    "        confirm_add = every_data_add[\"confirm\"]\n",
    "        suspect_add = every_data_add[\"suspect\"]\n",
    "        dead_add = every_data_add[\"dead\"]\n",
    "        heal_add = every_data_add[\"heal\"]\n",
    "        # g更新，update函数可以添加新的键值对\n",
    "        history_data[date_add].update({\"confirm_add\":confirm_add, \"suspect_add\":suspect_add, \"dead_add\":dead_add, \"heal_add\":heal_add})\n",
    "\n",
    "    ''' areaTree ：name 中国数据\n",
    "                   today\n",
    "                   total\n",
    "                   children :-name 省级数据 \n",
    "                            -today\n",
    "                            -total\n",
    "                            -children:-name 市级数据\n",
    "                                      -today\n",
    "                                      -total\n",
    "                            '''\n",
    "    details = []  #当日详细数据\n",
    "    update_time = data_today[\"lastUpdateTime\"]\n",
    "    data_country = data_today[\"areaTree\"]  #获取到中国国家\n",
    "    # print(data_country[0][\"children\"])\n",
    "    data_province = data_country[0][\"children\"]   #中国省份 \n",
    "    # print(data_province)\n",
    "    for pro_infos in data_province:\n",
    "        province = pro_infos[\"name\"]   #省名  \n",
    "        for city_infos in pro_infos[\"children\"]:  #获取市的名称\n",
    "            city = city_infos[\"name\"]\n",
    "            confirm = city_infos[\"total\"][\"confirm\"]\n",
    "            confirm_add = city_infos[\"today\"][\"confirm\"]\n",
    "            heal = city_infos[\"total\"][\"heal\"]\n",
    "            dead = city_infos[\"total\"][\"dead\"]\n",
    "            details.append([update_time, province, city, confirm, confirm_add, heal, dead])\n",
    "    return history_data, details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#存储数据 其中history 表存储每日总数据，details表存储每日详细数据\n",
    "# 安装pip install pymysql\n",
    "#(1)建立连接\n",
    "#（2）建立游标\n",
    "#（3）执行操作\n",
    "#（4）关闭连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接数据库\n",
    "def get_conn():   #封装连接\n",
    "    conn = pymysql.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"123456\",\n",
    "        db=\"cov1\",\n",
    "        charset=\"utf8\",\n",
    "        port=3306,\n",
    "    )\n",
    "    # 创建游标：\n",
    "    cursor = conn.cursor()  #执行完毕返回的结果集默认以元组显示\n",
    "    return conn, cursor \n",
    "#（2）关闭连接\n",
    "def close_conn(conn, cursor):   #执行完毕关闭连接和游标\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if conn:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#（3）定义更新细节数据函数\n",
    "def update_details(url_last, url_today):\n",
    "    cursor = None\n",
    "    conn = None\n",
    "    try:\n",
    "        # [['2020-07-01 15:20:46', '北京', '丰台', 266, 0, 43, 0],....]\n",
    "        detail_data = get_history(url_last, url_today)[1] # 0是字典数据 1是列表数据\n",
    "        conn, cursor = get_conn()  #创立连接，连接游标\n",
    "        #插入数据的sql语句，前面一开始的值，后面百分号做占位\n",
    "        sql = \"insert into details(update_time,province,city,confirm,confirm_add,heal,dead) values(%s,%s,%s,%s,%s,%s,%s)\"\n",
    "        #该sql获的是数据库的最新时间\n",
    "        sql_query = 'select %s=(select update_time from details order by id desc limit 1)' #对比当前最大时间戳,拿到最后一条数据\n",
    "        #对比当前最大时间戳\n",
    "        cursor.execute(sql_query, detail_data[0][0]) #这边的是从腾讯爬取出来的最新时间li[0]表示获取历史数据，li[0][0]获取的是历史数据的时间\n",
    "        if not cursor.fetchone()[0]:  #上述两个时间不同\n",
    "            print(f\"{time.asctime()}开始更新最新数据\")\n",
    "            for item in detail_data:\n",
    "                cursor.execute(sql, item)\n",
    "            conn.commit()  # 提交事务 update delete insert操作\n",
    "            print(f\"{time.asctime()}更新最新数据完毕\")\n",
    "        else:\n",
    "            print(f\"{time.asctime()}已是最新数据！\")\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        close_conn(conn, cursor)\n",
    "#插入历史数据\n",
    "def insert_history(url_last, url_today):\n",
    "    cursor = None\n",
    "    conn = None\n",
    "    try:\n",
    "        dic = get_history(url_last, url_today)[0] #0代表历史数据字典\n",
    "\n",
    "        print(f\"{time.asctime()}开始插入历史数据\")\n",
    "        conn, cursor = get_conn()\n",
    "        sql = \"insert into history values(%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "        for key, value in dic.items():\n",
    "             # item 格式 {'2020-01-13': {'confirm': 41, 'suspect': 0, 'heal': 0, 'dead': 1}\n",
    "            cursor.execute(sql, [key, value.get(\"confirm\"), value.get(\"confirm_add\"), value.get(\"suspect\"),\n",
    "                                 value.get(\"suspect_add\"), value.get(\"heal\"), value.get(\"heal_add\"),\n",
    "                                 value.get(\"dead\"), value.get(\"dead_add\")])\n",
    "        conn.commit()  # 提交事务 update delete insert操作\n",
    "        print(f\"{time.asctime()}插入历史数据完毕\")\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        close_conn(conn, cursor)\n",
    "\n",
    "#更新历史数据        \n",
    "def update_history(url_last, url_today):\n",
    "    cursor = None\n",
    "    conn = None\n",
    "    try:\n",
    "        dic = get_history(url_last, url_today)[0]  #0代表历史数据字典\n",
    "        print(f\"{time.asctime()}开始更新历史数据\")\n",
    "        conn, cursor = get_conn()\n",
    "        sql = \"insert into history values(%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "        sql_query = \"select confirm from history where ds=%s\"\n",
    "        for key, value in dic.items():\n",
    "            # item 格式 {'2020-01-13': {'confirm': 41, 'suspect': 0, 'heal': 0, 'dead': 1}\n",
    "            if not cursor.execute(sql_query, key):\n",
    "                cursor.execute(sql, [key, value.get(\"confirm\"), value.get(\"confirm_add\"), value.get(\"suspect\"),\n",
    "                                     value.get(\"suspect_add\"), value.get(\"heal\"), value.get(\"heal_add\"),\n",
    "                                     value.get(\"dead\"), value.get(\"dead_add\")])\n",
    "        conn.commit()\n",
    "        print(f\"{time.asctime()}历史数据更新完毕\")\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        close_conn(conn, cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov  2 20:30:38 2021开始更新最新数据\n",
      "Tue Nov  2 20:30:39 2021更新最新数据完毕\n",
      "Tue Nov  2 20:30:39 2021开始更新历史数据\n",
      "Tue Nov  2 20:30:41 2021历史数据更新完毕\n"
     ]
    }
   ],
   "source": [
    "update_details(url_last, url_today)  #更新历史数据\n",
    "update_history(url_last, url_today)  #插入历史数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬取百度热搜并保存到sql里面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import sys\n",
    "import pymysql\n",
    "import json\n",
    "import traceback\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写入数据库\n",
    "#爬取百度热搜数据\n",
    "def get_baidu_hot():\n",
    "    url = \"https://top.baidu.com/board?tab=realtime\"\n",
    "    path=\"D:\\Data\\jupyterNote Data\\chromedriver.exe\"  #可以指定驱动的位置，也可以放在.py同级目录下就不用设置path了\n",
    "    option=webdriver.ChromeOptions()\n",
    "    option.add_argument('--headless') #隐藏浏览器\n",
    "    option.add_argument(\"--no-sandbox\")\n",
    "    browser=webdriver.Chrome(path,options=option)\n",
    "    \n",
    "\n",
    "    \n",
    "    browser.get(url)  # 2.打开地址\n",
    "    time.sleep(1)\n",
    "    # 爬虫与反爬，模拟人等待1秒\n",
    "    c = browser.find_elements_by_xpath('//*[@id=\"sanRoot\"]/main/div[2]/div/div[2]/div/div[2]/a/div[1]')  #共有30条记录\n",
    "    context = [i.text for i in c]  #i.text获取标签内容，插入到list中来\n",
    "    print(context)\n",
    "    browser.close()\n",
    "    return context\n",
    "\n",
    "\n",
    "def update_hotsearch():\n",
    "    cursor = None\n",
    "    conn = None\n",
    "    try:\n",
    "        context = get_baidu_hot()  #得到内容\n",
    "        print(f\"{time.asctime()}开始更新数据\")\n",
    "        conn,cursor = get_conn() #拿取到了连接\n",
    "        sql = \"insert into hotsearch(dt,content) values(%s,%s)\" \n",
    "        ts = time.strftime(\"%Y-%m-%d %X\")\n",
    "        for i in context:\n",
    "            cursor.execute(sql,(ts,i))  #插入数据\n",
    "        conn.commit()  #提交事务保存数据\n",
    "        print(f\"{time.asctime()}数据更新完毕\")\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        close_conn(conn,cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['鼓励储存必需品引热议 商务部回应', '全能型冷空气将横扫 南北都要冻哭', '美核潜艇撞上海底山?外交部回应', '上海迪士尼11月3日起恢复运营', '渤海银行涉事员工目前已失联', '成都新增确诊与兰州病例有轨迹重叠', '马斯克发中文七步诗', '600亿基金经理蔡向阳身故 年仅41岁', '重庆九龙坡区新增1例确诊', '莫迪承诺印度2070年将实现净零排放', '济南市民收到战备应急包 官方回应', '宁夏吴忠回应男子发狗狗表情包被拘', '北京就限票政策“误伤”道歉', '河北石家庄进入应急状态', '江苏回应发布家庭应急物资清单', '钟南山:中国防疫政策并非成本过高', '雅虎在中国大陆停止产品与服务', '北京一家4口确诊 1人每日接送儿子', '拜登替特朗普政府向全世界道歉', '叶问3制片人施建祥在美被捕', '#李诚儒痛批娱乐圈娘炮文化#', '#范丞丞空降聊致命愿望#', '平顶山回应公交司机集体停运讨薪', '神十三航天员起床洗漱画面曝光', '事业单位招聘不得限制毕业院校', '迪士尼归来游客称自费隔离要6千', '耿爽用中国俗话正告个别国家', '重返英超!孔蒂执教热刺', '16省538例 百人规模疫情为何频发', '高校食堂男生因抢座狂扇女生耳光']\n",
      "Tue Nov  2 20:30:47 2021开始更新数据\n",
      "Tue Nov  2 20:30:49 2021数据更新完毕\n"
     ]
    }
   ],
   "source": [
    "update_hotsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
